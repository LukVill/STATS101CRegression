---
title: "Stats 101C - Regression Project"
author: "Instructions"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["amsmath"]
  html_document: default
---
```{r, echo = FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(tidymodels)
library(stringr)
```


# Gameplan:

  - vfold CV the training data
  - Try multiple models: multiple linear, tree, random forest, boosted tree, 
  - try multiple recipes: transformations on data
  - make models tunable with different parameters
  - make workflows with the different combinations of recipes and models AND different parameters
  - top 5 best result on assessment data will move on to the testing data
  - find best combination of model, recipe, parameters for the testing data


Getting initial glimpse of the data:

```{r}

# SET THE WD TO YOUR FOLDER

trainFilepath <- paste0(getwd(),"/train.csv")
testFilepath <- paste0(getwd(),"/test.csv")
columnDescFilepath <- paste0(getwd(),"/column_descriptions.csv")

train <- read.csv(trainFilepath)
test <- read.csv(testFilepath)
column_desc <- read.csv(columnDescFilepath)

```

```{r personal insights, message=F, eval=F}

glimpse(train)
glimpse(test)
view(column_desc)

```

```{r visualization of data}

summary(train)

```

Data has big min's and max's, so there might be some need for sample stratification OR transformation.

```{r transforming data via recipes}


```

```{r v fold}

# CROSS VALIDATE TRAIN TO AVOID OVERFITTING

# set seed for reproducibility
set.seed(101)

# going to use 233 partitions so there would be close to 10 used for assessment
train_split <- vfold_cv(train, v = 233)

```

```{r model creation}

linear_model <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")

tree_model <- decision_tree(min_n = tune(), cost_complexity = tune(), tree_depth = tune()) %>% set_engine("rpart") %>% set_mode("regression")

r_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% set_engine("ranger") %>% set_mode("regression")

boost_tree_model <- boost_tree(mtry = tune(), trees = tune(), min_n = tune(), learn_rate = tune()) %>% set_engine("xgboost")

```

Possible models: linear, decision tree, random forest, boosted tree