---
title: "Stats 101C - Regression Project"
author: "Instructions"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["amsmath"]
  html_document: default
---
```{r, echo = FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(tidymodels)
library(stringr)
library(corrplot)
```


# Gameplan:

  - vfold CV the training data
  - Try multiple models: multiple linear, tree, random forest, boosted tree, 
  - try multiple recipes: transformations on data
  - make models tunable with different parameters
  - make workflows with the different combinations of recipes and models AND different parameters
  - top 5 best result on assessment data will move on to the testing data
  - find best combination of model, recipe, parameters for the testing data
    - find lowest RMSE and highest RSQ 


Getting initial glimpse of the data:

```{r}

# SET THE WD TO YOUR FOLDER

trainFilepath <- paste0(getwd(),"/train.csv")
testFilepath <- paste0(getwd(),"/test.csv")
columnDescFilepath <- paste0(getwd(),"/column_descriptions.csv")

train <- read.csv(trainFilepath)
test <- read.csv(testFilepath)
column_desc <- read.csv(columnDescFilepath)

```

```{r personal insights, message=F, eval=F}

glimpse(train)
glimpse(test)
view(column_desc)

```

```{r visualization of data}

summary(train)

ncol(train)

```

Response variable has a skewed right distribution (median is on the lower end). Possibly should normalize the response variable.

Data has big min's and max's, so there might be some need for sample stratification OR transformation. Most likely a log transformation because the predictor is percentage, so the range is only from 0-1.

```{r transforming data via recipes}

# remove name from training data
train$name <- NULL

# possible transformations: log, normalization, boxcox

# offset by 1 because there are some 0's in the data

# remove na observations

# remove all highly correlated data to avoid multicollinearity that would affect ML models

# NOTE: ALL OF THESE RECIPES OFFSET THE DATA FIRST BY 1 BEFORE TRANSFORMING

rec_default <- recipe(percent_dem ~ ., data = train) %>% step_rm(id) %>% step_naomit(all_numeric_predictors()) %>% step_naomit(all_numeric_predictors()) %>% step_mutate_at(all_numeric_predictors(), fn = function(x){x + 1}) %>% step_corr(all_numeric_predictors(), threshold = 0.9)

rec_norm_resp <- rec_default %>% step_normalize(all_outcomes())

# make a function to attach pipeline to both default rec and normalized resp. rec
rec <- NULL

log_rec <- rec %>% step_log(all_numeric_predictors(), base = 10)

norm_rec <- rec %>% step_normalize(all_numeric_predictors())

log_norm_rec <- rec %>% step_log(all_numeric_predictors(), base = 10) %>% step_normalize(all_numeric_predictors())

norm_log_rec <- rec %>% step_normalize(all_numeric_predictors()) %>% step_log(all_numeric_predictors(), base = 10)

box_rec <- rec %>% step_BoxCox(all_numeric_predictors())

log_box_rec <- rec %>% step_log(all_numeric_predictors()) %>% step_BoxCox(all_numeric_predictors())

box_log_rec <- rec %>% step_BoxCox(all_numeric_predictors()) %>% step_log(all_numeric_predictors()) 

```

We are splitting the training data into 233 partitions to cross validate.

```{r v fold}

# CROSS VALIDATE TRAIN TO AVOID OVERFITTING

# set seed for reproducibility
set.seed(101)

# going to use 233 partitions so there would be close to 10 used for assessment
train_split <- vfold_cv(train, v = 233)

```

```{r model creation}

linear_model <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")

tree_model <- decision_tree(min_n = tune(), cost_complexity = tune(), tree_depth = tune()) %>% set_engine("rpart") %>% set_mode("regression")

r_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% set_engine("ranger") %>% set_mode("regression")

boost_tree_model <- boost_tree(mtry = tune(), trees = tune(), min_n = tune(), learn_rate = tune()) %>% set_engine("xgboost")

```

Possible models: linear, decision tree, random forest, boosted tree

```{r workflow creation}

# make list of recipes and models to combine different permutations in workflows
# make them work for CV resampling



```